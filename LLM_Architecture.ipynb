{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "B3SPY4bj-xev"
      },
      "outputs": [],
      "source": [
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,    # Vocabulary size\n",
        "    \"context_length\": 1024, # Context length\n",
        "    \"emb_dim\": 768,         # Embedding dimension\n",
        "    \"n_heads\": 12,          # Number of attention heads\n",
        "    \"n_layers\": 12,         # Number of layers\n",
        "    \"drop_rate\": 0.1,       # Dropout rate\n",
        "    \"qkv_bias\": False       # Query-Key-Value bias\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "class DummyGPTModel(nn.Module):\n",
        "  def __init__(self,cfg):\n",
        "    super().__init__()\n",
        "    self.tok_emb= nn.Embedding(cfg[\"vocab_size\"],cfg[\"emb_dim\"])\n",
        "    self.pos_emb= nn.Embedding(cfg[\"context_length\"],cfg[\"emb_dim\"])\n",
        "    self.drop_emb= nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    self.trf_blocks= nn.Sequential(\n",
        "        *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
        "    )\n",
        "    self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
        "    self.out_head= nn.Linear(cfg[\"emb_dim\"],cfg[\"vocab_size\"],bias=False)\n",
        "\n",
        "  def forward(self,in_idx):\n",
        "    batch_size,seq_len= in_idx.shape\n",
        "    tok_embeds= self.tok_emb(in_idx)\n",
        "    print(tok_embeds.shape)\n",
        "    pos_embeds= self.pos_emb(torch.arange(seq_len,device=in_idx.device))\n",
        "    print(pos_embeds.shape)\n",
        "    x= tok_embeds+pos_embeds\n",
        "    x= self.drop_emb(x)\n",
        "    x=self.trf_blocks(x)\n",
        "    x=self.final_norm(x)\n",
        "    logits=self.out_head(x)\n",
        "    return logits\n",
        "\n",
        "class DummyTransformerBlock(nn.Module):\n",
        "  def __init__(self,cfg):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self,x):\n",
        "    return x\n",
        "\n",
        "class DummyLayerNorm(nn.Module):\n",
        "  def __init__(self,normalized_shape,eps=1e-5):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self,x):\n",
        "    return x"
      ],
      "metadata": {
        "id": "JrY34sdF_xiz"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lj6zCGqX_xpO",
        "outputId": "317bfc8b-b68f-4b8f-b7d7-e8b72a15e443"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.7.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "tokenizer= tiktoken.get_encoding(\"gpt2\")\n",
        "batch=[]\n",
        "txt1=\"Every effort moves you\"\n",
        "txt2=\"Every day holds a\"\n",
        "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
        "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
        "batch = torch.stack(batch,dim=0)\n",
        "print(batch)"
      ],
      "metadata": {
        "id": "sDVUEZl-_xr7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fe37b0b-a14c-439e-c488-e125518f6614"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[6109, 3626, 6100,  345],\n",
            "        [6109, 1110, 6622,  257]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = DummyGPTModel(GPT_CONFIG_124M)\n",
        "logits= model(batch)\n",
        "print(logits)"
      ],
      "metadata": {
        "id": "GmQhCcbA_xvZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57b3b0fa-4ff5-4617-f1f1-9f0dbeb84325"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 4, 768])\n",
            "torch.Size([4, 768])\n",
            "tensor([[[-0.2535, -0.3889,  1.5002,  ...,  0.0708, -0.4895, -0.8134],\n",
            "         [ 0.8437,  1.1698,  0.3753,  ..., -0.3563,  0.0862,  1.0870],\n",
            "         [ 0.3511, -0.1863,  1.6779,  ...,  0.9093,  0.1901, -0.3551],\n",
            "         [ 0.2928,  1.6927,  0.3337,  ..., -1.5287,  1.0519,  0.0889]],\n",
            "\n",
            "        [[-0.2481, -0.6733,  1.2488,  ...,  0.1388, -0.6451, -0.1518],\n",
            "         [-0.6809, -0.0164, -0.1427,  ..., -0.6387, -0.4914,  0.1511],\n",
            "         [ 0.8166,  1.2937,  0.6436,  ..., -0.8961,  1.9247,  0.3397],\n",
            "         [-0.2927,  0.0989, -1.0180,  ..., -1.1201,  0.9358, -0.2313]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self,emb_dim):\n",
        "    super().__init__()\n",
        "    self.eps=1e-5\n",
        "    self.scale= nn.Parameter(torch.ones(emb_dim))\n",
        "    self.shift= nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "  def forward(self,x):\n",
        "    mean= x.mean(dim=-1,keepdim=True)\n",
        "    var= x.var(dim=-1,keepdim=True,unbiased=False)\n",
        "    norm_x= (x-mean)/torch.sqrt(var+self.eps)\n",
        "    return self.scale * norm_x + self.shift"
      ],
      "metadata": {
        "id": "-Wn6zjXdy2RW"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_example= torch.randn(2,5)\n",
        "layer= nn.Sequential(nn.Linear(5,6),nn.ReLU())\n",
        "out= layer(batch_example)\n",
        "print(out)\n",
        "print(batch_example)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeGIa6AMy2x3",
        "outputId": "87de82a6-e327-4003-eca3-6739ec8eae5e"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2756, 0.0000, 0.0000, 0.0781, 0.0000, 0.6024],\n",
            "        [0.5110, 0.0000, 0.0000, 0.0000, 0.8750, 0.0000]],\n",
            "       grad_fn=<ReluBackward0>)\n",
            "tensor([[-1.1605,  0.4938, -0.1759,  0.2549, -0.9464],\n",
            "        [-0.4554, -0.8502, -0.5240, -2.0425, -0.6820]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ln = LayerNorm(emb_dim=5)\n",
        "out_ln= ln(batch_example)\n",
        "mean= out_ln.mean(dim=-1,keepdim=True)\n",
        "var= out_ln.var(dim=-1,keepdim=True,unbiased=False)\n",
        "print(mean)\n",
        "print(var)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTz_Zh2Ny21X",
        "outputId": "101ca920-ae7d-48e1-a0db-23a606ebfbeb"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-1.1921e-08],\n",
            "        [ 0.0000e+00]], grad_fn=<MeanBackward1>)\n",
            "tensor([[1.0000],\n",
            "        [1.0000]], grad_fn=<VarBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GELU(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self,x):\n",
        "    return 0.5*x*(1+torch.tanh(torch.sqrt(torch.tensor(2.0/torch.pi)))*(x+0.044715*torch.pow(x,3)))"
      ],
      "metadata": {
        "id": "VnAYCZ5F_Ssz"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self,cfg):\n",
        "    super().__init__()\n",
        "    self.layers= nn.Sequential(\n",
        "        nn.Linear(cfg[\"emb_dim\"],4*cfg[\"emb_dim\"]),\n",
        "        GELU(),\n",
        "        nn.Linear(4*cfg[\"emb_dim\"],cfg[\"emb_dim\"])\n",
        "    )\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.layers(x)"
      ],
      "metadata": {
        "id": "MXjFGaIB_Sv9"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ffn=FeedForward(GPT_CONFIG_124M)\n",
        "x= torch.rand(3,2,768)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_cdIpSw_Szb",
        "outputId": "ffd7491f-7095-456d-8a8c-31e8335ca990"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.0563, 0.7045, 0.8232,  ..., 0.6787, 0.5999, 0.9497],\n",
            "         [0.8256, 0.7222, 0.5211,  ..., 0.8207, 0.0151, 0.7893]],\n",
            "\n",
            "        [[0.0714, 0.0829, 0.0808,  ..., 0.7235, 0.4363, 0.7851],\n",
            "         [0.8723, 0.3467, 0.2456,  ..., 0.2798, 0.8494, 0.2363]],\n",
            "\n",
            "        [[0.7729, 0.4657, 0.6138,  ..., 0.8990, 0.6104, 0.7986],\n",
            "         [0.2826, 0.1841, 0.5102,  ..., 0.1858, 0.7505, 0.5958]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out= ffn(x)\n",
        "print(out)\n",
        "print(out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJdg-5W0Bt0l",
        "outputId": "4ad68c2f-54de-4e07-8622-412c82d7874c"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.1300,  0.0582, -0.0312,  ..., -0.1637, -0.0172, -0.2211],\n",
            "         [ 0.0030, -0.0003, -0.0150,  ..., -0.1140, -0.0395, -0.1370]],\n",
            "\n",
            "        [[-0.0498, -0.0039,  0.0254,  ..., -0.0521, -0.0844, -0.2441],\n",
            "         [-0.0862, -0.0684, -0.0536,  ..., -0.1231, -0.0444, -0.1629]],\n",
            "\n",
            "        [[-0.0774,  0.0292, -0.1643,  ..., -0.0753, -0.1293, -0.2449],\n",
            "         [-0.0506,  0.1334, -0.0506,  ...,  0.0008, -0.0290, -0.1960]]],\n",
            "       grad_fn=<ViewBackward0>)\n",
            "torch.Size([3, 2, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#shortcut connections\n",
        "class ExampleDeepNeuralNetwork(nn.Module):\n",
        "  def __init__(self,layer_sizes,use_shortcut):\n",
        "    super().__init__()\n",
        "    self.use_shortcut=use_shortcut\n",
        "    self.layers= nn.ModuleList([\n",
        "        nn.Sequential(nn.Linear(layer_sizes[0],layer_sizes[1]),GELU()),\n",
        "        nn.Sequential(nn.Linear(layer_sizes[1],layer_sizes[2]),GELU()),\n",
        "        nn.Sequential(nn.Linear(layer_sizes[2],layer_sizes[3]),GELU()),\n",
        "        nn.Sequential(nn.Linear(layer_sizes[3],layer_sizes[4]),GELU()),\n",
        "        nn.Sequential(nn.Linear(layer_sizes[4],layer_sizes[5]),GELU()),\n",
        "    ])\n",
        "\n",
        "  def forward(self,x):\n",
        "    for layer in self.layers:\n",
        "      layer_output= layer(x)\n",
        "      if self.use_shortcut and x.shape==layer_output.shape:\n",
        "        x=x+layer_output\n",
        "      else:\n",
        "        x= layer_output\n",
        "\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "pH6AQ0psNFAH"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer_sizes=[3,3,3,3,3,1]\n",
        "sample_input= torch.tensor([[1.,0.,-1.]])\n",
        "model_without_shortcut= ExampleDeepNeuralNetwork(\n",
        "    layer_sizes,use_shortcut=False\n",
        ")"
      ],
      "metadata": {
        "id": "-cy4GNy1NFFc"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_gradients(model,x):\n",
        "  output= model(x)\n",
        "  target= torch.tensor([[0.]])\n",
        "  loss=nn.MSELoss()\n",
        "  loss=loss(output,target)\n",
        "  loss.backward()\n",
        "  for name,param in model.named_parameters():\n",
        "    if 'weight' in name:\n",
        "      print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")"
      ],
      "metadata": {
        "id": "QEZJcBQLNFH8"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(print_gradients(model_without_shortcut,sample_input))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2sq5poJUNFKs",
        "outputId": "bdcc6b11-e6bb-47a3-b818-cf6f168eb75e"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "layers.0.0.weight has gradient mean of 4.243206058163196e-05\n",
            "layers.1.0.weight has gradient mean of 3.883019962813705e-05\n",
            "layers.2.0.weight has gradient mean of 0.00016868537932168692\n",
            "layers.3.0.weight has gradient mean of 0.0001764867192832753\n",
            "layers.4.0.weight has gradient mean of 0.0031230065505951643\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_without_shortcut= ExampleDeepNeuralNetwork(\n",
        "    layer_sizes,use_shortcut=True\n",
        ")\n",
        "print(print_gradients(model_without_shortcut,sample_input))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEV33HwjNFOP",
        "outputId": "764ba383-0fbb-4ce8-ad69-eb5dfb7c3b5a"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "layers.0.0.weight has gradient mean of 0.0030419996473938227\n",
            "layers.1.0.weight has gradient mean of 0.005177794024348259\n",
            "layers.2.0.weight has gradient mean of 0.0069147138856351376\n",
            "layers.3.0.weight has gradient mean of 0.008061745204031467\n",
            "layers.4.0.weight has gradient mean of 0.07722031325101852\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(GPT_CONFIG_124M)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYyDyQ4FjCBV",
        "outputId": "ade908a5-a430-4a77-eb51-c614d7e7d0b9"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'vocab_size': 50257, 'context_length': 1024, 'emb_dim': 768, 'n_heads': 12, 'n_layers': 12, 'drop_rate': 0.1, 'qkv_bias': False}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self,d_in,d_out,context_length,dropout,num_heads,qkv_bias=False):\n",
        "    super().__init__()\n",
        "    assert(d_out%num_heads==0),\\\n",
        "    \"d_out must be divisible by num_heads\"\n",
        "    self.d_out= d_out\n",
        "    self.num_heads= num_heads\n",
        "    self.head_dim= d_out//num_heads\n",
        "    self.W_query= nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "    self.W_key= nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "    self.W_value= nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "    self.out_proj=nn.Linear(d_out,d_out) #linear layer to combine head outputs\n",
        "    self.dropout= nn.Dropout(dropout)\n",
        "    self.register_buffer(\"mask\",torch.triu(torch.ones(context_length,context_length),diagonal=1))\n",
        "\n",
        "  def forward(self,x):\n",
        "    b,num_tokens,d_in= x.shape\n",
        "    keys= self.W_key(x) # (b,num_tokens,d_out)\n",
        "    queries= self.W_query(x)\n",
        "    values= self.W_value(x)\n",
        "    # we implicitly split matrix by adding num_heads dim\n",
        "    keys= keys.view(b,num_tokens,self.num_heads,self.head_dim)\n",
        "    queries= queries.view(b,num_tokens,self.num_heads,self.head_dim)\n",
        "    values= values.view(b,num_tokens,self.num_heads,self.head_dim)\n",
        "    #transpose (b,num_tokens,num_heads,head_dim)->(b,num_heads,num_tokens,head_dim)\n",
        "    #This dimension helps in parallel computation\n",
        "    keys= keys.transpose(1,2)\n",
        "    queries=queries.transpose(1,2)\n",
        "    values= values.transpose(1,2)\n",
        "    attn_scores= queries @ keys.transpose(2,3) #(b,num_heads,num_tokens,num_tokens)\n",
        "    mask_bool= self.mask.bool()[:num_tokens,:num_tokens]\n",
        "    attn_scores.masked_fill_(mask_bool,-torch.inf)\n",
        "    attn_weights= torch.softmax(attn_scores/keys.shape[-1]**0.5,dim=-1)\n",
        "    attn_weights= self.dropout(attn_weights)\n",
        "    #context vec before tanspose (b,num_heads,num_token,head_dim)\n",
        "    #after transpose (b,num_tokens,num_heads,head_dim)\n",
        "    #merging the heads back\n",
        "    context_vec= (attn_weights @ values).transpose(1,2)\n",
        "    context_vec= context_vec.contiguous().view(b,num_tokens,self.d_out)\n",
        "    context_vec= self.out_proj(context_vec)\n",
        "    return context_vec\n"
      ],
      "metadata": {
        "id": "PcWertaJhv6D"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self,cfg):\n",
        "    super().__init__()\n",
        "    self.att= MultiHeadAttention(\n",
        "        d_in= cfg[\"emb_dim\"],\n",
        "        d_out= cfg[\"emb_dim\"],\n",
        "        context_length= cfg[\"context_length\"],\n",
        "        num_heads= cfg[\"n_heads\"],\n",
        "        dropout= cfg[\"drop_rate\"],\n",
        "        qkv_bias= cfg[\"qkv_bias\"]\n",
        "    )\n",
        "    self.ff= FeedForward(cfg)\n",
        "    self.norm1= LayerNorm(cfg[\"emb_dim\"])\n",
        "    self.norm2= LayerNorm(cfg[\"emb_dim\"])\n",
        "    self.drop_shortcut= nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "  def forward(self,x):\n",
        "    shortcut=x\n",
        "    x= self.norm1(x)\n",
        "    x= self.att(x)\n",
        "    x= self.drop_shortcut(x)\n",
        "    x=shortcut+x\n",
        "\n",
        "    shortcut=x\n",
        "    x= self.norm2(x)\n",
        "    x= self.ff(x)\n",
        "    x= self.drop_shortcut(x)\n",
        "    x=x+shortcut\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "pXxnMZAsjB-q"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x= torch.rand(2,4,768)\n",
        "block = TransformerBlock(GPT_CONFIG_124M)\n",
        "output= block(x)\n",
        "print(\"Input shape:\",x.shape)\n",
        "print(\"Output shape:\",output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwpAa8rijCD9",
        "outputId": "8cdd0c7b-ec7b-4458-a34a-b54f1c64c451"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([2, 4, 768])\n",
            "Output shape: torch.Size([2, 4, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTModel(nn.Module):\n",
        "  def __init__(self,cfg):\n",
        "    super().__init__()\n",
        "    self.tok_emb= nn.Embedding(cfg[\"vocab_size\"],cfg[\"emb_dim\"])\n",
        "    self.pos_emb= nn.Embedding(cfg[\"context_length\"],cfg[\"emb_dim\"])\n",
        "    self.drop_emb= nn.Dropout(cfg[\"drop_rate\"])\n",
        "    self.trf_blocks=nn.Sequential(\n",
        "        *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n",
        "    )\n",
        "    self.final_norm= LayerNorm(cfg[\"emb_dim\"])\n",
        "    self.out_head= nn.Linear(cfg[\"emb_dim\"],cfg[\"vocab_size\"],bias=False)\n",
        "\n",
        "  def forward(self,in_idx):\n",
        "    batch_size,seq_len= in_idx.shape\n",
        "    tok_embeds= self.tok_emb(in_idx)\n",
        "    pos_embeds= self.pos_emb(torch.arange(seq_len,device=in_idx.device))\n",
        "    x= tok_embeds+pos_embeds\n",
        "    x=self.drop_emb(x)\n",
        "    x= self.trf_blocks(x)\n",
        "    x= self.final_norm(x)\n",
        "    logits= self.out_head(x)\n",
        "    return logits"
      ],
      "metadata": {
        "id": "HPP9b2dejCGp"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3WJwOCNpdIE",
        "outputId": "0e6428cf-2656-4cae-e0f0-c35e624993be"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[6109, 3626, 6100,  345],\n",
            "        [6109, 1110, 6622,  257]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model= GPTModel(GPT_CONFIG_124M)\n",
        "out= model(batch)\n",
        "print(\"input shape:\",batch.shape)\n",
        "print(\"output shape:\",out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZFKnHjdjCJW",
        "outputId": "c225d68e-f031-4c16-d233-043388e58eba"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input shape: torch.Size([2, 4])\n",
            "output shape: torch.Size([2, 4, 50257])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_params= sum(p.numel() for p in model.parameters())\n",
        "print(\"Total parameters:\",total_params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ppu5Ep7NjCMC",
        "outputId": "cd4e43ae-63b1-4613-a8e9-1f281604a82e"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total parameters: 163009536\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
        "print(\"Output layer shape:\", model.out_head.weight.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9wB-4KxjCPn",
        "outputId": "78113062-991a-40d5-bf7d-cf56eabb8de8"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token embedding layer shape: torch.Size([50257, 768])\n",
            "Output layer shape: torch.Size([50257, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_params_gpt2 = total_params - sum(p.numel() for p in model.out_head.parameters())\n",
        "print(f\"Number of trainable parameters considering weight tying: {total_params_gpt2:,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpl-fcLIrqEU",
        "outputId": "1bb2c5d5-a0f7-4b2d-9b52-049e5fb41ff9"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of trainable parameters considering weight tying: 124,412,160\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_size_bytes = total_params * 4 #A\n",
        "total_size_mb = total_size_bytes / (1024 * 1024) #B\n",
        "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9ENxj0jr8Fj",
        "outputId": "c73778d4-9e86-4577-81b5-462f6c532e26"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total size of the model: 621.83 MB\n"
          ]
        }
      ]
    }
  ]
}